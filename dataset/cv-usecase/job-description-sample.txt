Role: Build, design and, implement our highly scalable, fault-tolerant big data platform to process terabytes of data and provide customers with in depth analytics. Developing Big Data pipelines using modern technology stacks such as Spark, Hadoop, Kafka, HBase, Hive, and Presto. Developing analytics applications using modern technology stacks such as Java, Spring, Tomcat, Jenkins, REST APIs, JDBC, Amazon Web Services, Hibernate. Building data pipeline to automate high volume collection and processing to provide realtime data analytics. Customize reporting and analytics platform based on customer’s requirements from customers and deliver scalable, production ready solutions. Lead multiple projects to develop features for data processing and reporting platforms, collaborate with product managers, and cross functional teams. Collaborate with functional teams to build products to deliver end-to-end products and features and fix bugs for better performance. Develop robust & fault tolerant systems and monitor implications of changes on data processing pipeline and performance. Leveraging a broad range of data architecture strategies and proposing both data flows and storage solutions. Managing Hadoop map reduce and spark jobs and solving any ongoing issues with operating the cluster. Working closely with cross functional teams on improving the availability and scalability of large data platforms and the functionality of PubMatic software. Participate in Agile/Scrum processes such as sprint planning, sprint retrospective, backlog grooming, user story management, and work item prioritization. Frequently discuss with product managers about the software features to include in PubMatic Data Analytics platform. Support customer issues over email or JIRA (bug tracking system), provide updates, patches to customers to fix the issues. Perform code and design reviews for code implemented by peers or as per the code review process. | Skills: Proficiency in Python, Java; experience with databases; familiarity with Agile methodologies. | Experience: 6+ years of proven experience in designing, implementing and delivering complex, scalable and resilient platform and services. Experience in building high throughput big data platforms and systems. Hands-on experience in big data technologies (Spark/Kafka/Spark streaming) and other open source data technologies. Experience in OLAP (Snowflake, Vertica or similar) would be an added advantage.Ability to understand vague business problems and convert into working solutions. Excellent spoken and written interpersonal skills with a collaborative approach. Dedication to developing high-quality software and products. Curiosity to explore and understand data is a strong plus. Deep understanding of Big-Data and distributed systems (MapReduce, Spark, Hive, Kafka, Oozie, Airflow) | Education: Bachelor’s degree in Computer Science or related field.